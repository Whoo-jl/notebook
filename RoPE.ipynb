{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def sinusoidal_position_embedding(batch_size, nums_head, max_len, output_dim, device):\n",
    "    # (max_len, 1)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(-1)\n",
    "    # (output_dim // 2), 公式中的i，i的取值范围为(0, d/2)\n",
    "    ids = torch.arange(0, output_dim //2, dtype=torch.float)  # 区别奇数项与偶数项\n",
    "    theta = torch.pow(10000, -2*ids/output_dim)\n",
    "    # (max_len, output_dim/2)\n",
    "    embedding = position * theta # 公式定义 pos/ 10000^(2i/d)\n",
    "    # (max_len, output_dim // 2, 2)\n",
    "    embedding = torch.stack([torch.sin(embedding), torch.cos(embedding)], dim=-1)\n",
    "    # (bs, head, max_len, output_dim //2, 2)\n",
    "    embedding = embedding.repeat((batch_size, nums_head, *([1] * len(embedding.shape))))\n",
    "\n",
    "    # (ba, head, max_len, output_dim), reshape之后就是sin和cos\n",
    "    embedding = torch.reshape(embedding, (batch_size, nums_head, max_len, output_dim))\n",
    "    embedding = embedding.to(device)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def RoPE(q, k):\n",
    "    batch_size, nums_head, max_len, output_dim = q.shape\n",
    "    pos_emb = sinusoidal_position_embedding(batch_size, nums_head, max_len, output_dim, q.device)\n",
    "\n",
    "    # cos_pos,sin_pos: (bs, head, max_len, output_dim)\n",
    "    # 看rope公式可知，相邻cos，sin之间是相同的，所以复制一遍。如(1,2,3)变成(1,1,2,2,3,3)\n",
    "    cos_pos = pos_emb[...,  1::2].repeat_interleave(2, dim=-1)  # 将奇数列信息抽取出来也就是cos 拿出来并复制\n",
    "    sin_pos = pos_emb[..., ::2].repeat_interleave(2, dim=-1)  # 将偶数列信息抽取出来也就是sin 拿出来并复制\n",
    "\n",
    "    # q,k: (bs, head, max_len, output_dim)\n",
    "    q2 = torch.stack([-q[..., 1::2], q[..., ::2]], dim=-1)\n",
    "    q2 = q2.reshape(q.shape)  # reshape后就是正负交替了\n",
    "\n",
    "\n",
    "    # 更新qw, *对应位置相乘\n",
    "    q = q * cos_pos + q2 * sin_pos\n",
    "\n",
    "    k2 = torch.stack([-k[..., 1::2], k[..., ::2]], dim=-1)\n",
    "    k2 = k2.reshape(k.shape)\n",
    "    # 更新kw, *对应位置相乘\n",
    "    k = k * cos_pos + k2 * sin_pos\n",
    "\n",
    "    return q, k\n",
    "\n",
    "\n",
    "def attention(q, k, v, mask=None, dropout=None, use_RoPE=True):\n",
    "    if use_RoPE:\n",
    "        q, k = RoPE(q, k)\n",
    "    d_k = k.size()[-1]\n",
    "\n",
    "    att_logits = torch.matmul(q, k.transpose(-2, -1))  # (bs, head, seq_len, seq_len)\n",
    "    att_logits /= math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        att_logits = att_logits.masked_fill(mask == 0, -1e9)  # mask掉为0的部分，设为无穷大\n",
    "\n",
    "    att_scores = F.softmax(att_logits, dim=-1)  # (bs, head, seq_len, seq_len)\n",
    "\n",
    "    if dropout is not None:\n",
    "        att_scores = dropout(att_scores)\n",
    "\n",
    "    # (bs, head, seq_len, seq_len) * (bs, head, seq_len, dk) = (bs, head, seq_len, dk)\n",
    "    return torch.matmul(att_scores, v), att_scores\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # (bs, head, seq_len, dk)\n",
    "    q = torch.randn(8, 12, 10, 32)\n",
    "    k = torch.randn(8, 12, 10, 32)\n",
    "    v = torch.randn(8, 12, 10, 32)\n",
    "\n",
    "    res, att_score = attention(q, k, v, mask=None, dropout=None, use_RoPE=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
