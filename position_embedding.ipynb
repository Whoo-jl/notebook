{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 512])\n",
      "torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: 词向量的维度\n",
    "            max_len: 位置嵌入的最大序列长度\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # 初始化位置嵌入矩阵\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # 位置索引（0, 1, ..., max_len-1）\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # 计算频率项\n",
    "\n",
    "        # 偶数维使用sin，奇数维使用cos\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数位置：sin\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # 奇数位置：cos\n",
    "        \n",
    "        # 添加额外的维度使其适合与输入相加，并冻结参数\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)  # Shape: (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)  # 将pe注册为buffer，确保在训练时不会被更新\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 输入张量，形状为 (seq_len, batch_size, d_model)\n",
    "        \"\"\"\n",
    "        # 将位置嵌入与输入相加\n",
    "        print(self.pe[:x.size(0), :].shape)\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "# 测试位置嵌入\n",
    "seq_len, batch_size, d_model = 10, 32, 512  # 定义序列长度、批量大小和嵌入维度\n",
    "x = torch.zeros(seq_len, batch_size, d_model)  # 创建一个零输入\n",
    "pos_encoder = PositionalEncoding(d_model)  # 初始化位置编码器\n",
    "x = pos_encoder(x)  # 应用位置嵌入\n",
    "\n",
    "print(x.shape)  # 输出 (seq_len, batch_size, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [8.],\n",
      "        [9.]])\n",
      "tensor([  0,   2,   4,   6,   8,  10,  12,  14,  16,  18,  20,  22,  24,  26,\n",
      "         28,  30,  32,  34,  36,  38,  40,  42,  44,  46,  48,  50,  52,  54,\n",
      "         56,  58,  60,  62,  64,  66,  68,  70,  72,  74,  76,  78,  80,  82,\n",
      "         84,  86,  88,  90,  92,  94,  96,  98, 100, 102, 104, 106, 108, 110,\n",
      "        112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138,\n",
      "        140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166,\n",
      "        168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194,\n",
      "        196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222,\n",
      "        224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250,\n",
      "        252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278,\n",
      "        280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306,\n",
      "        308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334,\n",
      "        336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362,\n",
      "        364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390,\n",
      "        392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418,\n",
      "        420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446,\n",
      "        448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474,\n",
      "        476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502,\n",
      "        504, 506, 508, 510])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "d_model = 512\n",
    "max_len = 10\n",
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "a = torch.arange(0, d_model, 2)\n",
    "print(position)\n",
    "print(a)\n",
    "(a*position).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
